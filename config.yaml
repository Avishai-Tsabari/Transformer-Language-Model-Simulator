hparams:
  vocab_size: 8000  # Maximal allowed vocabulary size for both word-level and BPE tokenization. Everything above will be truncated.
  d_model: 8
  num_heads: 1
  num_layers: 2
  d_ff: 32 # perferable 4X d_model
  sequence_length: 64 
  stride: 1 # if stride is equal to sequence_length, the dataset will be created with no overlap, if the stride is equal to 1, the dataset will be created with full overlap
  batch_size: 32
  learning_rate: 0.0005 # 0.0001
  max_epochs: 100
  patience: 10
  min_delta: 0.001
  warmup_steps: 1000
  weight_decay: 0.01
  dropout: 0.2
  train_split: 0.7  # Adjusted to give more data to validation
  val_split: 0.2   # Increased validation split
  num_workers: 0
  accelerator: "auto" # "auto" | "gpu" | "cpu"
  devices: 1
  precision: "32"
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  log_every_n_steps: 50
  val_check_interval: 1.0
  save_top_k: 1
  monitor: "val/loss"
  mode: "min"

# Tokenizer configuration
tokenizer:
  type: "word"  # Options: "word" or "bpe"
  # Word tokenizer options (used when type="word")
  word_options: {}
  
  # BPE tokenizer options (used when type="bpe")
  bpe_options:
    min_frequency: 2      # Minimum frequency for a merge to be considered
    special_tokens: ["<PAD>", "<UNK>", "<SOS>", "<EOS>"]

paths:
  corpus_path: "sample_corpus.txt"
  save_dir: "./checkpoints"
  log_dir: "./logs"

general:
  experiment_name: "transformer_lm"
  create_sample: True