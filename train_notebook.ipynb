{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fe2ba7",
   "metadata": {},
   "source": [
    "# **Transformer Language Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a71c15",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5022e",
   "metadata": {},
   "source": [
    "#### Set Root Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9045cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Use the current working directory as root (or go up if needed)\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '../..'))  # Adjust '..' as needed\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73e3ac",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a95daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training script for the transformer language model.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "import yaml\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from src.data.tokenizer import SimpleTokenizer, load_text_corpus, create_sample_corpus\n",
    "from src.data.dataset import TextDataModule\n",
    "from src.model.lightning_module import TransformerLightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a82678",
   "metadata": {},
   "source": [
    "## Load Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef2483",
   "metadata": {},
   "source": [
    "#### Load yaml config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b1aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be0eb5",
   "metadata": {},
   "source": [
    "#### Set hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = config[\"hparams\"]\n",
    "\n",
    "# this is not the best way to do unpack the hyperparams, but it's just for the demo\n",
    "vocab_size, d_model, num_heads, num_layers, d_ff, \\\n",
    "sequence_length, batch_size, learning_rate, max_epochs, \\\n",
    "patience, min_delta, warmup_steps, weight_decay, dropout, \\\n",
    "train_split, val_split, num_workers, accelerator, devices, precision, \\\n",
    "gradient_clip_val, accumulate_grad_batches, log_every_n_steps, \\\n",
    "val_check_interval, save_top_k, monitor, mode = hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c227a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 3000,\n",
       " 'd_model': 32,\n",
       " 'num_heads': 1,\n",
       " 'num_layers': 2,\n",
       " 'd_ff': 1024,\n",
       " 'sequence_length': 32,\n",
       " 'batch_size': 64,\n",
       " 'learning_rate': 0.0001,\n",
       " 'max_epochs': 100,\n",
       " 'patience': 10,\n",
       " 'min_delta': 0.001,\n",
       " 'warmup_steps': 1000,\n",
       " 'weight_decay': 0.01,\n",
       " 'dropout': 0.1,\n",
       " 'train_split': 0.7,\n",
       " 'val_split': 0.2,\n",
       " 'num_workers': 0,\n",
       " 'accelerator': 'auto',\n",
       " 'devices': 1,\n",
       " 'precision': '32',\n",
       " 'gradient_clip_val': 1.0,\n",
       " 'accumulate_grad_batches': 1,\n",
       " 'log_every_n_steps': 50,\n",
       " 'val_check_interval': 1.0,\n",
       " 'save_top_k': 1,\n",
       " 'monitor': 'val/loss',\n",
       " 'mode': 'min'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9dbddb",
   "metadata": {},
   "source": [
    "#### Set paths and general config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e94d5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = config[\"paths\"]\n",
    "general = config[\"general\"]\n",
    "\n",
    "corpus_path=paths[\"corpus_path\"]\n",
    "save_dir=paths[\"save_dir\"]\n",
    "log_dir=paths[\"log_dir\"]\n",
    "experiment_name=general[\"experiment_name\"]\n",
    "create_sample=general[\"create_sample\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc99cda",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911beab1",
   "metadata": {},
   "source": [
    "#### Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b474cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and tokenizing text...\n"
     ]
    }
   ],
   "source": [
    "# Load and tokenize text\n",
    "print(\"Loading and tokenizing text...\")\n",
    "text = load_text_corpus(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26fac3",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b4426c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built with 2059 tokens\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer and build vocabulary\n",
    "tokenizer = SimpleTokenizer(vocab_size=hparams[\"vocab_size\"])\n",
    "tokenizer.build_vocab(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb77f3",
   "metadata": {},
   "source": [
    "#### Save vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to ./checkpoints\\vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save vocabulary.\n",
    "# For the demo, we override the vocab file. you can adjust the vocal file name as you like.\n",
    "vocab_path = os.path.join(save_dir, \"vocab.pkl\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "tokenizer.save_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ac026",
   "metadata": {},
   "source": [
    "#### Read vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e0575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2059"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read vocab\n",
    "# this step is only necessary if loading the vocab from a file instead of creating one\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(\"The number of tokens in the vocab is:\", len(vocab['word_to_idx']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f693192",
   "metadata": {},
   "source": [
    "#### Get token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd86e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text length: 21110 tokens\n"
     ]
    }
   ],
   "source": [
    "# Encode text\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Encoded text length: {len(token_ids)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd27c60",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f1611",
   "metadata": {},
   "source": [
    "#### Create data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3531c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "data_module = TextDataModule(\n",
    "    token_ids=token_ids,\n",
    "    sequence_length=sequence_length,\n",
    "    batch_size=batch_size,\n",
    "    train_split=train_split,\n",
    "    val_split=val_split,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25cbd6",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e160cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = TransformerLightningModule(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf90a3",
   "metadata": {},
   "source": [
    "#### Set callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0db2eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    patience=patience,\n",
    "    min_delta=min_delta,\n",
    "    mode=mode,\n",
    "    verbose=True\n",
    ")\n",
    "callbacks.append(early_stopping)\n",
    "\n",
    "\n",
    "# Model checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=save_dir,\n",
    "    filename=f\"{experiment_name}-epoch={{epoch:02d}}-val_loss={{val/loss:.3f}}\", #-v{trainer.logger.version:02d}\",\n",
    "    monitor=monitor,\n",
    "    mode=mode,\n",
    "    auto_insert_metric_name=False, # Prevents the name 'val/loss=' from being prepended\n",
    "    save_top_k=save_top_k,\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "# Learning rate monitoring\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "callbacks.append(lr_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b673769",
   "metadata": {},
   "source": [
    "#### Create trainer and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e196f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "# Create logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=log_dir,\n",
    "    name=experiment_name,\n",
    "    version=None\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    precision=precision,\n",
    "    max_epochs=max_epochs,\n",
    "    gradient_clip_val=gradient_clip_val,\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    log_every_n_steps=log_every_n_steps,\n",
    "    val_check_interval=val_check_interval,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    deterministic=True,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c1c0b",
   "metadata": {},
   "source": [
    "#### Summarize prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary:\n",
      "Vocabulary size: 2059\n",
      "Model parameters: 275,723\n",
      "Trainable parameters: 275,723\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(\"\\nModel Summary:\")\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Check data sizes\n",
    "print(\"Data sizes:\")\n",
    "print(f\"Total tokens: {len(token_ids)}\")\n",
    "print(f\"Sequence length: {sequence_length}\")\n",
    "print(f\"Train split: {train_split}\")\n",
    "print(f\"Val split: {val_split}\")\n",
    "\n",
    "# Calculate split sizes\n",
    "total_len = len(token_ids)\n",
    "train_end = int(total_len * train_split)\n",
    "val_end = int(total_len * (train_split + val_split))\n",
    "\n",
    "print(f\"Train tokens: {train_end}\")\n",
    "print(f\"Val tokens: {val_end - train_end}\")\n",
    "print(f\"Test tokens: {total_len - val_end}\")\n",
    "\n",
    "# Check if validation data is sufficient\n",
    "val_tokens = val_end - train_end\n",
    "if val_tokens < sequence_length:\n",
    "    print(f\"WARNING: Validation data has only {val_tokens} tokens, less than sequence length {sequence_length}\")\n",
    "    print(\"This will cause validation to fail. Consider using a larger corpus or adjusting splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efc6f7",
   "metadata": {},
   "source": [
    "#### Train the neural netork transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c88831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "With the updated parameters:\n",
      "- Sequence length: 32\n",
      "- Train split: 0.7\n",
      "- Val split: 0.2\n",
      "- Monitor: val/loss\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:751: Checkpoint directory C:\\code\\data_science\\demo_llm\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | TransformerLM    | 275 K  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "275 K     Trainable params\n",
      "0         Non-trainable params\n",
      "275 K     Total params\n",
      "1.103     Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 3/231 [00:00<00:22, 10.03it/s, v_num=29, train/loss_step=7.790, train/perplexity_step=2.43e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 231/231 [00:21<00:00, 10.66it/s, v_num=29, train/loss_step=7.610, train/perplexity_step=2.02e+3, val/loss=7.550, val/perplexity=1.91e+3, train/loss_epoch=7.710, train/perplexity_epoch=2.23e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved. New best score: 7.553\n",
      "Epoch 0, global step 231: 'val/loss' reached 7.55281 (best 7.55281), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=00-val_loss=7.553.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 231/231 [00:21<00:00, 10.67it/s, v_num=29, train/loss_step=7.230, train/perplexity_step=1.38e+3, val/loss=7.180, val/perplexity=1.32e+3, train/loss_epoch=7.390, train/perplexity_epoch=1.63e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.368 >= min_delta = 0.001. New best score: 7.184\n",
      "Epoch 1, global step 462: 'val/loss' reached 7.18438 (best 7.18438), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=01-val_loss=7.184.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 231/231 [00:21<00:00, 10.71it/s, v_num=29, train/loss_step=6.760, train/perplexity_step=859.0, val/loss=6.750, val/perplexity=859.0, train/loss_epoch=7.000, train/perplexity_epoch=1.11e+3]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.436 >= min_delta = 0.001. New best score: 6.748\n",
      "Epoch 2, global step 693: 'val/loss' reached 6.74838 (best 6.74838), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=02-val_loss=6.748.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 231/231 [00:21<00:00, 10.56it/s, v_num=29, train/loss_step=6.380, train/perplexity_step=592.0, val/loss=6.330, val/perplexity=571.0, train/loss_epoch=6.570, train/perplexity_epoch=717.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.416 >= min_delta = 0.001. New best score: 6.332\n",
      "Epoch 3, global step 924: 'val/loss' reached 6.33216 (best 6.33216), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=03-val_loss=6.332.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 231/231 [00:21<00:00, 10.66it/s, v_num=29, train/loss_step=6.010, train/perplexity_step=406.0, val/loss=6.060, val/perplexity=438.0, train/loss_epoch=6.210, train/perplexity_epoch=500.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.275 >= min_delta = 0.001. New best score: 6.057\n",
      "Epoch 4, global step 1155: 'val/loss' reached 6.05727 (best 6.05727), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=04-val_loss=6.057.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 231/231 [00:21<00:00, 10.67it/s, v_num=29, train/loss_step=5.990, train/perplexity_step=398.0, val/loss=5.900, val/perplexity=376.0, train/loss_epoch=5.980, train/perplexity_epoch=397.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.159 >= min_delta = 0.001. New best score: 5.898\n",
      "Epoch 5, global step 1386: 'val/loss' reached 5.89824 (best 5.89824), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=05-val_loss=5.898.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 231/231 [00:22<00:00, 10.37it/s, v_num=29, train/loss_step=5.850, train/perplexity_step=347.0, val/loss=5.810, val/perplexity=347.0, train/loss_epoch=5.850, train/perplexity_epoch=347.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.086 >= min_delta = 0.001. New best score: 5.812\n",
      "Epoch 6, global step 1617: 'val/loss' reached 5.81225 (best 5.81225), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=06-val_loss=5.812.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 231/231 [00:23<00:00,  9.90it/s, v_num=29, train/loss_step=5.650, train/perplexity_step=283.0, val/loss=5.780, val/perplexity=337.0, train/loss_epoch=5.780, train/perplexity_epoch=325.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.029 >= min_delta = 0.001. New best score: 5.783\n",
      "Epoch 7, global step 1848: 'val/loss' reached 5.78303 (best 5.78303), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=07-val_loss=5.783.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 231/231 [00:22<00:00, 10.12it/s, v_num=29, train/loss_step=5.770, train/perplexity_step=320.0, val/loss=5.780, val/perplexity=336.0, train/loss_epoch=5.770, train/perplexity_epoch=320.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.002 >= min_delta = 0.001. New best score: 5.781\n",
      "Epoch 8, global step 2079: 'val/loss' reached 5.78053 (best 5.78053), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=08-val_loss=5.781.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 231/231 [00:23<00:00,  9.99it/s, v_num=29, train/loss_step=5.770, train/perplexity_step=321.0, val/loss=5.760, val/perplexity=331.0, train/loss_epoch=5.760, train/perplexity_epoch=318.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.016 >= min_delta = 0.001. New best score: 5.765\n",
      "Epoch 9, global step 2310: 'val/loss' reached 5.76462 (best 5.76462), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=09-val_loss=5.765.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 231/231 [00:24<00:00,  9.62it/s, v_num=29, train/loss_step=5.710, train/perplexity_step=302.0, val/loss=5.700, val/perplexity=311.0, train/loss_epoch=5.720, train/perplexity_epoch=305.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.064 >= min_delta = 0.001. New best score: 5.700\n",
      "Epoch 10, global step 2541: 'val/loss' reached 5.70016 (best 5.70016), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=10-val_loss=5.700.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 231/231 [00:23<00:00,  9.89it/s, v_num=29, train/loss_step=5.610, train/perplexity_step=274.0, val/loss=5.580, val/perplexity=276.0, train/loss_epoch=5.610, train/perplexity_epoch=275.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.121 >= min_delta = 0.001. New best score: 5.579\n",
      "Epoch 11, global step 2772: 'val/loss' reached 5.57901 (best 5.57901), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=11-val_loss=5.579.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 231/231 [00:23<00:00,  9.77it/s, v_num=29, train/loss_step=5.480, train/perplexity_step=239.0, val/loss=5.420, val/perplexity=237.0, train/loss_epoch=5.460, train/perplexity_epoch=236.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.157 >= min_delta = 0.001. New best score: 5.422\n",
      "Epoch 12, global step 3003: 'val/loss' reached 5.42237 (best 5.42237), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=12-val_loss=5.422.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 231/231 [00:23<00:00,  9.84it/s, v_num=29, train/loss_step=5.200, train/perplexity_step=182.0, val/loss=5.280, val/perplexity=205.0, train/loss_epoch=5.290, train/perplexity_epoch=198.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.146 >= min_delta = 0.001. New best score: 5.276\n",
      "Epoch 13, global step 3234: 'val/loss' reached 5.27596 (best 5.27596), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=13-val_loss=5.276.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 231/231 [00:23<00:00,  9.79it/s, v_num=29, train/loss_step=5.070, train/perplexity_step=159.0, val/loss=5.170, val/perplexity=185.0, train/loss_epoch=5.140, train/perplexity_epoch=172.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.104 >= min_delta = 0.001. New best score: 5.172\n",
      "Epoch 14, global step 3465: 'val/loss' reached 5.17244 (best 5.17244), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=14-val_loss=5.172.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 231/231 [00:23<00:00,  9.80it/s, v_num=29, train/loss_step=4.820, train/perplexity_step=124.0, val/loss=5.120, val/perplexity=175.0, train/loss_epoch=5.050, train/perplexity_epoch=157.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.052 >= min_delta = 0.001. New best score: 5.121\n",
      "Epoch 15, global step 3696: 'val/loss' reached 5.12085 (best 5.12085), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=15-val_loss=5.121.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 231/231 [00:24<00:00,  9.61it/s, v_num=29, train/loss_step=5.100, train/perplexity_step=163.0, val/loss=5.110, val/perplexity=173.0, train/loss_epoch=5.020, train/perplexity_epoch=151.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.013 >= min_delta = 0.001. New best score: 5.107\n",
      "Epoch 16, global step 3927: 'val/loss' reached 5.10749 (best 5.10749), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=16-val_loss=5.107.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 231/231 [00:23<00:00,  9.88it/s, v_num=29, train/loss_step=4.910, train/perplexity_step=136.0, val/loss=5.110, val/perplexity=173.0, train/loss_epoch=5.010, train/perplexity_epoch=150.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.002 >= min_delta = 0.001. New best score: 5.105\n",
      "Epoch 17, global step 4158: 'val/loss' reached 5.10539 (best 5.10539), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=17-val_loss=5.105.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 231/231 [00:23<00:00,  9.88it/s, v_num=29, train/loss_step=4.840, train/perplexity_step=127.0, val/loss=5.080, val/perplexity=169.0, train/loss_epoch=5.000, train/perplexity_epoch=149.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.023 >= min_delta = 0.001. New best score: 5.083\n",
      "Epoch 18, global step 4389: 'val/loss' reached 5.08266 (best 5.08266), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=18-val_loss=5.083.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 231/231 [00:23<00:00, 10.01it/s, v_num=29, train/loss_step=4.920, train/perplexity_step=137.0, val/loss=5.020, val/perplexity=160.0, train/loss_epoch=4.960, train/perplexity_epoch=142.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.059 >= min_delta = 0.001. New best score: 5.024\n",
      "Epoch 19, global step 4620: 'val/loss' reached 5.02382 (best 5.02382), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=19-val_loss=5.024.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 231/231 [00:24<00:00,  9.61it/s, v_num=29, train/loss_step=4.600, train/perplexity_step=99.30, val/loss=4.930, val/perplexity=145.0, train/loss_epoch=4.860, train/perplexity_epoch=130.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.098 >= min_delta = 0.001. New best score: 4.926\n",
      "Epoch 20, global step 4851: 'val/loss' reached 4.92613 (best 4.92613), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=20-val_loss=4.926.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 231/231 [00:23<00:00,  9.64it/s, v_num=29, train/loss_step=4.810, train/perplexity_step=123.0, val/loss=4.820, val/perplexity=130.0, train/loss_epoch=4.740, train/perplexity_epoch=115.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.108 >= min_delta = 0.001. New best score: 4.818\n",
      "Epoch 21, global step 5082: 'val/loss' reached 4.81781 (best 4.81781), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=21-val_loss=4.818.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 231/231 [00:24<00:00,  9.31it/s, v_num=29, train/loss_step=4.610, train/perplexity_step=101.0, val/loss=4.720, val/perplexity=119.0, train/loss_epoch=4.620, train/perplexity_epoch=102.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.095 >= min_delta = 0.001. New best score: 4.723\n",
      "Epoch 22, global step 5313: 'val/loss' reached 4.72301 (best 4.72301), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=22-val_loss=4.723.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 231/231 [00:24<00:00,  9.58it/s, v_num=29, train/loss_step=4.590, train/perplexity_step=98.80, val/loss=4.660, val/perplexity=111.0, train/loss_epoch=4.530, train/perplexity_epoch=93.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.063 >= min_delta = 0.001. New best score: 4.660\n",
      "Epoch 23, global step 5544: 'val/loss' reached 4.66025 (best 4.66025), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=23-val_loss=4.660.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 231/231 [00:24<00:00,  9.49it/s, v_num=29, train/loss_step=4.480, train/perplexity_step=87.90, val/loss=4.630, val/perplexity=108.0, train/loss_epoch=4.480, train/perplexity_epoch=88.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.029 >= min_delta = 0.001. New best score: 4.631\n",
      "Epoch 24, global step 5775: 'val/loss' reached 4.63098 (best 4.63098), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=24-val_loss=4.631.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 231/231 [00:23<00:00,  9.71it/s, v_num=29, train/loss_step=4.560, train/perplexity_step=96.10, val/loss=4.630, val/perplexity=108.0, train/loss_epoch=4.460, train/perplexity_epoch=86.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.004 >= min_delta = 0.001. New best score: 4.627\n",
      "Epoch 25, global step 6006: 'val/loss' reached 4.62712 (best 4.62712), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=25-val_loss=4.627.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 231/231 [00:23<00:00,  9.83it/s, v_num=29, train/loss_step=4.430, train/perplexity_step=83.60, val/loss=4.620, val/perplexity=107.0, train/loss_epoch=4.460, train/perplexity_epoch=86.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.004 >= min_delta = 0.001. New best score: 4.623\n",
      "Epoch 26, global step 6237: 'val/loss' reached 4.62298 (best 4.62298), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=26-val_loss=4.623.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 231/231 [00:23<00:00,  9.76it/s, v_num=29, train/loss_step=4.390, train/perplexity_step=80.50, val/loss=4.600, val/perplexity=105.0, train/loss_epoch=4.440, train/perplexity_epoch=85.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.026 >= min_delta = 0.001. New best score: 4.597\n",
      "Epoch 27, global step 6468: 'val/loss' reached 4.59729 (best 4.59729), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=27-val_loss=4.597.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 231/231 [00:23<00:00, 10.01it/s, v_num=29, train/loss_step=4.490, train/perplexity_step=88.70, val/loss=4.540, val/perplexity=99.00, train/loss_epoch=4.400, train/perplexity_epoch=81.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.057 >= min_delta = 0.001. New best score: 4.541\n",
      "Epoch 28, global step 6699: 'val/loss' reached 4.54076 (best 4.54076), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=28-val_loss=4.541.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 231/231 [00:24<00:00,  9.61it/s, v_num=29, train/loss_step=4.410, train/perplexity_step=82.20, val/loss=4.460, val/perplexity=91.90, train/loss_epoch=4.320, train/perplexity_epoch=75.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.076 >= min_delta = 0.001. New best score: 4.465\n",
      "Epoch 29, global step 6930: 'val/loss' reached 4.46455 (best 4.46455), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=29-val_loss=4.465.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 231/231 [00:24<00:00,  9.61it/s, v_num=29, train/loss_step=4.260, train/perplexity_step=70.70, val/loss=4.380, val/perplexity=84.60, train/loss_epoch=4.230, train/perplexity_epoch=68.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.083 >= min_delta = 0.001. New best score: 4.382\n",
      "Epoch 30, global step 7161: 'val/loss' reached 4.38196 (best 4.38196), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=30-val_loss=4.382.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 231/231 [00:23<00:00,  9.75it/s, v_num=29, train/loss_step=4.240, train/perplexity_step=69.60, val/loss=4.320, val/perplexity=79.30, train/loss_epoch=4.140, train/perplexity_epoch=63.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.067 >= min_delta = 0.001. New best score: 4.315\n",
      "Epoch 31, global step 7392: 'val/loss' reached 4.31522 (best 4.31522), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=31-val_loss=4.315.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 231/231 [00:23<00:00,  9.85it/s, v_num=29, train/loss_step=4.050, train/perplexity_step=57.40, val/loss=4.270, val/perplexity=76.20, train/loss_epoch=4.080, train/perplexity_epoch=59.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.040 >= min_delta = 0.001. New best score: 4.275\n",
      "Epoch 32, global step 7623: 'val/loss' reached 4.27484 (best 4.27484), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=32-val_loss=4.275.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 231/231 [00:23<00:00, 10.03it/s, v_num=29, train/loss_step=4.340, train/perplexity_step=76.50, val/loss=4.260, val/perplexity=75.20, train/loss_epoch=4.050, train/perplexity_epoch=57.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.013 >= min_delta = 0.001. New best score: 4.262\n",
      "Epoch 33, global step 7854: 'val/loss' reached 4.26185 (best 4.26185), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=33-val_loss=4.262.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 231/231 [00:23<00:00,  9.78it/s, v_num=29, train/loss_step=3.770, train/perplexity_step=43.50, val/loss=4.260, val/perplexity=75.10, train/loss_epoch=4.050, train/perplexity_epoch=57.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.002 >= min_delta = 0.001. New best score: 4.260\n",
      "Epoch 34, global step 8085: 'val/loss' reached 4.26035 (best 4.26035), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=34-val_loss=4.260.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 231/231 [00:23<00:00,  9.65it/s, v_num=29, train/loss_step=3.810, train/perplexity_step=45.30, val/loss=4.250, val/perplexity=74.60, train/loss_epoch=4.040, train/perplexity_epoch=57.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.007 >= min_delta = 0.001. New best score: 4.253\n",
      "Epoch 35, global step 8316: 'val/loss' reached 4.25294 (best 4.25294), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=35-val_loss=4.253.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 231/231 [00:23<00:00, 10.04it/s, v_num=29, train/loss_step=4.120, train/perplexity_step=61.60, val/loss=4.230, val/perplexity=72.60, train/loss_epoch=4.020, train/perplexity_epoch=56.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.027 >= min_delta = 0.001. New best score: 4.226\n",
      "Epoch 36, global step 8547: 'val/loss' reached 4.22573 (best 4.22573), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=36-val_loss=4.226.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 231/231 [00:23<00:00,  9.86it/s, v_num=29, train/loss_step=3.980, train/perplexity_step=53.60, val/loss=4.170, val/perplexity=68.90, train/loss_epoch=3.980, train/perplexity_epoch=53.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.052 >= min_delta = 0.001. New best score: 4.174\n",
      "Epoch 37, global step 8778: 'val/loss' reached 4.17367 (best 4.17367), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=37-val_loss=4.174.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 231/231 [00:23<00:00,  9.82it/s, v_num=29, train/loss_step=3.950, train/perplexity_step=52.20, val/loss=4.110, val/perplexity=64.80, train/loss_epoch=3.920, train/perplexity_epoch=50.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.063 >= min_delta = 0.001. New best score: 4.111\n",
      "Epoch 38, global step 9009: 'val/loss' reached 4.11093 (best 4.11093), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=38-val_loss=4.111.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 231/231 [00:23<00:00,  9.78it/s, v_num=29, train/loss_step=3.580, train/perplexity_step=35.80, val/loss=4.050, val/perplexity=60.70, train/loss_epoch=3.850, train/perplexity_epoch=47.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.065 >= min_delta = 0.001. New best score: 4.045\n",
      "Epoch 39, global step 9240: 'val/loss' reached 4.04546 (best 4.04546), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=39-val_loss=4.045.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 231/231 [00:22<00:00, 10.42it/s, v_num=29, train/loss_step=3.920, train/perplexity_step=50.20, val/loss=4.000, val/perplexity=58.20, train/loss_epoch=3.790, train/perplexity_epoch=44.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.044 >= min_delta = 0.001. New best score: 4.001\n",
      "Epoch 40, global step 9471: 'val/loss' reached 4.00099 (best 4.00099), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=40-val_loss=4.001.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 231/231 [00:23<00:00,  9.91it/s, v_num=29, train/loss_step=3.780, train/perplexity_step=43.70, val/loss=3.970, val/perplexity=56.60, train/loss_epoch=3.750, train/perplexity_epoch=42.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.026 >= min_delta = 0.001. New best score: 3.975\n",
      "Epoch 41, global step 9702: 'val/loss' reached 3.97452 (best 3.97452), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=41-val_loss=3.975.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 231/231 [00:23<00:00,  9.77it/s, v_num=29, train/loss_step=3.560, train/perplexity_step=35.30, val/loss=3.970, val/perplexity=56.30, train/loss_epoch=3.730, train/perplexity_epoch=41.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.005 >= min_delta = 0.001. New best score: 3.970\n",
      "Epoch 42, global step 9933: 'val/loss' reached 3.96956 (best 3.96956), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=42-val_loss=3.970.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 231/231 [00:23<00:00,  9.84it/s, v_num=29, train/loss_step=3.900, train/perplexity_step=49.20, val/loss=3.970, val/perplexity=56.30, train/loss_epoch=3.730, train/perplexity_epoch=41.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.001 >= min_delta = 0.001. New best score: 3.969\n",
      "Epoch 43, global step 10164: 'val/loss' reached 3.96854 (best 3.96854), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=43-val_loss=3.969.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 231/231 [00:23<00:00,  9.68it/s, v_num=29, train/loss_step=3.870, train/perplexity_step=47.70, val/loss=3.960, val/perplexity=55.60, train/loss_epoch=3.720, train/perplexity_epoch=41.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.013 >= min_delta = 0.001. New best score: 3.956\n",
      "Epoch 44, global step 10395: 'val/loss' reached 3.95567 (best 3.95567), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=44-val_loss=3.956.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 231/231 [00:24<00:00,  9.47it/s, v_num=29, train/loss_step=3.680, train/perplexity_step=39.80, val/loss=3.930, val/perplexity=54.10, train/loss_epoch=3.700, train/perplexity_epoch=40.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.026 >= min_delta = 0.001. New best score: 3.930\n",
      "Epoch 45, global step 10626: 'val/loss' reached 3.92970 (best 3.92970), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=45-val_loss=3.930.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 231/231 [00:23<00:00,  9.73it/s, v_num=29, train/loss_step=3.770, train/perplexity_step=43.40, val/loss=3.880, val/perplexity=51.70, train/loss_epoch=3.660, train/perplexity_epoch=39.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.046 >= min_delta = 0.001. New best score: 3.884\n",
      "Epoch 46, global step 10857: 'val/loss' reached 3.88404 (best 3.88404), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=46-val_loss=3.884.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 231/231 [00:24<00:00,  9.59it/s, v_num=29, train/loss_step=3.580, train/perplexity_step=36.00, val/loss=3.830, val/perplexity=48.90, train/loss_epoch=3.610, train/perplexity_epoch=37.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.057 >= min_delta = 0.001. New best score: 3.827\n",
      "Epoch 47, global step 11088: 'val/loss' reached 3.82738 (best 3.82738), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=47-val_loss=3.827.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 231/231 [00:24<00:00,  9.31it/s, v_num=29, train/loss_step=3.440, train/perplexity_step=31.10, val/loss=3.780, val/perplexity=46.60, train/loss_epoch=3.560, train/perplexity_epoch=35.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.049 >= min_delta = 0.001. New best score: 3.778\n",
      "Epoch 48, global step 11319: 'val/loss' reached 3.77803 (best 3.77803), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=48-val_loss=3.778.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 231/231 [00:23<00:00,  9.86it/s, v_num=29, train/loss_step=3.540, train/perplexity_step=34.40, val/loss=3.740, val/perplexity=45.00, train/loss_epoch=3.520, train/perplexity_epoch=33.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.034 >= min_delta = 0.001. New best score: 3.744\n",
      "Epoch 49, global step 11550: 'val/loss' reached 3.74386 (best 3.74386), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=49-val_loss=3.744.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 231/231 [00:24<00:00,  9.42it/s, v_num=29, train/loss_step=3.370, train/perplexity_step=28.90, val/loss=3.730, val/perplexity=44.40, train/loss_epoch=3.490, train/perplexity_epoch=32.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.013 >= min_delta = 0.001. New best score: 3.731\n",
      "Epoch 50, global step 11781: 'val/loss' reached 3.73067 (best 3.73067), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=50-val_loss=3.731.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 231/231 [00:23<00:00,  9.79it/s, v_num=29, train/loss_step=3.700, train/perplexity_step=40.60, val/loss=3.730, val/perplexity=44.30, train/loss_epoch=3.480, train/perplexity_epoch=32.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.003 >= min_delta = 0.001. New best score: 3.728\n",
      "Epoch 51, global step 12012: 'val/loss' reached 3.72776 (best 3.72776), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=51-val_loss=3.728.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 231/231 [00:23<00:00,  9.99it/s, v_num=29, train/loss_step=3.760, train/perplexity_step=42.90, val/loss=3.730, val/perplexity=44.20, train/loss_epoch=3.480, train/perplexity_epoch=32.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.001 >= min_delta = 0.001. New best score: 3.726\n",
      "Epoch 52, global step 12243: 'val/loss' reached 3.72641 (best 3.72641), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=52-val_loss=3.726.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 231/231 [00:23<00:00,  9.80it/s, v_num=29, train/loss_step=3.540, train/perplexity_step=34.50, val/loss=3.710, val/perplexity=43.60, train/loss_epoch=3.480, train/perplexity_epoch=32.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.014 >= min_delta = 0.001. New best score: 3.712\n",
      "Epoch 53, global step 12474: 'val/loss' reached 3.71234 (best 3.71234), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=53-val_loss=3.712.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 231/231 [00:23<00:00,  9.67it/s, v_num=29, train/loss_step=3.490, train/perplexity_step=32.90, val/loss=3.690, val/perplexity=42.40, train/loss_epoch=3.450, train/perplexity_epoch=31.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.027 >= min_delta = 0.001. New best score: 3.685\n",
      "Epoch 54, global step 12705: 'val/loss' reached 3.68521 (best 3.68521), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=54-val_loss=3.685.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 231/231 [00:23<00:00,  9.63it/s, v_num=29, train/loss_step=3.290, train/perplexity_step=26.70, val/loss=3.640, val/perplexity=40.50, train/loss_epoch=3.420, train/perplexity_epoch=30.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.045 >= min_delta = 0.001. New best score: 3.640\n",
      "Epoch 55, global step 12936: 'val/loss' reached 3.64021 (best 3.64021), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=55-val_loss=3.640.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 231/231 [00:23<00:00,  9.75it/s, v_num=29, train/loss_step=3.320, train/perplexity_step=27.80, val/loss=3.590, val/perplexity=38.60, train/loss_epoch=3.380, train/perplexity_epoch=29.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.047 >= min_delta = 0.001. New best score: 3.593\n",
      "Epoch 56, global step 13167: 'val/loss' reached 3.59294 (best 3.59294), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=56-val_loss=3.593.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 231/231 [00:23<00:00,  9.72it/s, v_num=29, train/loss_step=3.240, train/perplexity_step=25.60, val/loss=3.560, val/perplexity=37.20, train/loss_epoch=3.340, train/perplexity_epoch=28.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.038 >= min_delta = 0.001. New best score: 3.555\n",
      "Epoch 57, global step 13398: 'val/loss' reached 3.55540 (best 3.55540), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=57-val_loss=3.555.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 231/231 [00:24<00:00,  9.58it/s, v_num=29, train/loss_step=3.220, train/perplexity_step=25.00, val/loss=3.530, val/perplexity=36.40, train/loss_epoch=3.310, train/perplexity_epoch=27.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.022 >= min_delta = 0.001. New best score: 3.533\n",
      "Epoch 58, global step 13629: 'val/loss' reached 3.53296 (best 3.53296), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=58-val_loss=3.533.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 231/231 [00:23<00:00,  9.86it/s, v_num=29, train/loss_step=3.140, train/perplexity_step=23.20, val/loss=3.520, val/perplexity=36.10, train/loss_epoch=3.290, train/perplexity_epoch=27.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.008 >= min_delta = 0.001. New best score: 3.525\n",
      "Epoch 59, global step 13860: 'val/loss' reached 3.52454 (best 3.52454), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=59-val_loss=3.525.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60:  26%|██▌       | 59/231 [00:05<00:17,  9.91it/s, v_num=29, train/loss_step=3.280, train/perplexity_step=26.50, val/loss=3.520, val/perplexity=36.10, train/loss_epoch=3.290, train/perplexity_epoch=27.00] "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"With the updated parameters:\")\n",
    "print(f\"- Sequence length: {sequence_length}\")\n",
    "print(f\"- Train split: {train_split}\")\n",
    "print(f\"- Val split: {val_split}\")\n",
    "print(f\"- Monitor: {monitor}\")\n",
    "print()\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6a938",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ed8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model...\n",
      "Testing DataLoader 0: 100%|██████████| 19/19 [00:00<00:00, 53.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     4.971528053283691     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    4.971528053283691    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to ./checkpoints\\transformer_lm-final.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "print(\"\\nTesting model...\")\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save best model instead of final model\n",
    "if checkpoint_callback.best_model_path:\n",
    "    # Copy the best model to a final location with version number\n",
    "    import shutil\n",
    "    version = trainer.logger.version\n",
    "    final_model_path = os.path.join(save_dir, f\"{experiment_name}-best-v{version:02d}.ckpt\")\n",
    "    shutil.copy2(checkpoint_callback.best_model_path, final_model_path)\n",
    "    print(f\"Best model copied to {final_model_path}\")\n",
    "else:\n",
    "    print(\"No best model found, saving current model as final\")\n",
    "    version = trainer.logger.version\n",
    "    final_model_path = os.path.join(save_dir, f\"{experiment_name}-final-v{version:02d}.ckpt\")\n",
    "    trainer.save_checkpoint(final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc3ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Checkpoints saved in: ./checkpoints\n",
      "Logs saved in: ./logs\n",
      "Vocabulary saved in: ./checkpoints\\vocab.pkl\n",
      "Best model: C:\\code\\data_science\\demo_llm\\checkpoints\\transformer_lm-epoch=40-val_loss=5.686.ckpt\n",
      "Best score: 5.686227798461914\n",
      "\n",
      "==================================================\n",
      "To run the Gradio app with your trained model:\n",
      "python -m src.app.gradio_app --model_path C:\\code\\data_science\\demo_llm\\checkpoints\\transformer_lm-epoch=40-val_loss=5.686.ckpt --vocab_path ./checkpoints\\vocab.pkl\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Checkpoints saved in: {save_dir}\")\n",
    "print(f\"Logs saved in: {log_dir}\")\n",
    "print(f\"Vocabulary saved in: {vocab_path}\")\n",
    "print(f\"Best model: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best score: {checkpoint_callback.best_model_score}\")\n",
    "print(f\"Final model saved as: {final_model_path}\")\n",
    "\n",
    "# Print instructions for running the app\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"To run the Gradio app with your trained model:\")\n",
    "print(f\"python -m src.app.gradio_app --model_path {final_model_path} --vocab_path {vocab_path}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43b33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
