{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fe2ba7",
   "metadata": {},
   "source": [
    "# **Transformer Language Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a71c15",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7faf2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device() if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5022e",
   "metadata": {},
   "source": [
    "#### Set Root Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9045cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Use the current working directory as root (or go up if needed)\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '../..'))  # Adjust '..' as needed\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73e3ac",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a95daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training script for the transformer language model.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "import yaml\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from src.data.tokenizer import SimpleTokenizer, load_text_corpus, create_sample_corpus\n",
    "from src.data.tokenizer_factory import TokenizerFactory\n",
    "from src.data.dataset import TextDataModule\n",
    "from src.model.lightning_module import TransformerLightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a82678",
   "metadata": {},
   "source": [
    "## Load Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef2483",
   "metadata": {},
   "source": [
    "#### Load yaml config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b1aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be0eb5",
   "metadata": {},
   "source": [
    "#### Set hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2122d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = config[\"hparams\"]\n",
    "\n",
    "# this is not the best way to do unpack the hyperparams, but it's just for the demo\n",
    "vocab_size, d_model, num_heads, num_layers, d_ff, \\\n",
    "sequence_length, stride, batch_size, learning_rate, max_epochs, \\\n",
    "patience, min_delta, warmup_steps, weight_decay, dropout, \\\n",
    "train_split, val_split, num_workers, accelerator, devices, precision, \\\n",
    "gradient_clip_val, accumulate_grad_batches, log_every_n_steps, \\\n",
    "val_check_interval, save_top_k, monitor, mode = hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c227a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 8000,\n",
       " 'd_model': 8,\n",
       " 'num_heads': 1,\n",
       " 'num_layers': 2,\n",
       " 'd_ff': 32,\n",
       " 'sequence_length': 64,\n",
       " 'stride': 1,\n",
       " 'batch_size': 32,\n",
       " 'learning_rate': 0.0005,\n",
       " 'max_epochs': 100,\n",
       " 'patience': 10,\n",
       " 'min_delta': 0.001,\n",
       " 'warmup_steps': 1000,\n",
       " 'weight_decay': 0.01,\n",
       " 'dropout': 0.2,\n",
       " 'train_split': 0.7,\n",
       " 'val_split': 0.2,\n",
       " 'num_workers': 0,\n",
       " 'accelerator': 'auto',\n",
       " 'devices': 1,\n",
       " 'precision': '32',\n",
       " 'gradient_clip_val': 1.0,\n",
       " 'accumulate_grad_batches': 1,\n",
       " 'log_every_n_steps': 50,\n",
       " 'val_check_interval': 1.0,\n",
       " 'save_top_k': 1,\n",
       " 'monitor': 'val/loss',\n",
       " 'mode': 'min'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9dbddb",
   "metadata": {},
   "source": [
    "#### Set paths and general config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94d5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = config[\"paths\"]\n",
    "general = config[\"general\"]\n",
    "\n",
    "corpus_path=paths[\"corpus_path\"]\n",
    "save_dir=paths[\"save_dir\"]\n",
    "log_dir=paths[\"log_dir\"]\n",
    "experiment_name=general[\"experiment_name\"]\n",
    "create_sample=general[\"create_sample\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc99cda",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911beab1",
   "metadata": {},
   "source": [
    "#### Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b474cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and tokenizing text...\n"
     ]
    }
   ],
   "source": [
    "# Load and tokenize text\n",
    "print(\"Loading and tokenizing text...\")\n",
    "text = load_text_corpus(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26fac3",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a450797",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create tokenizer and build vocabulary\n",
    "    tokenizer_config = config.get(\"tokenizer\", {})\n",
    "    tokenizer_type = tokenizer_config.get(\"type\", \"word\")\n",
    "    \n",
    "    if tokenizer_type == \"word\":\n",
    "        tokenizer = TokenizerFactory.create_tokenizer(\"word\", vocab_size=vocab_size)\n",
    "    elif tokenizer_type == \"bpe\":\n",
    "        bpe_options = tokenizer_config.get(\"bpe_options\", {})\n",
    "        tokenizer = TokenizerFactory.create_tokenizer(\"bpe\", vocab_size=vocab_size, **bpe_options)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tokenizer type: {tokenizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b4426c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built with 8002 tokens\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer and build vocabulary\n",
    "# tokenizer = SimpleTokenizer(vocab_size=hparams[\"vocab_size\"])\n",
    "tokenizer.build_vocab(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb77f3",
   "metadata": {},
   "source": [
    "#### Save vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary.\n",
    "# For the demo, we override the vocab file. you can adjust the vocal file name as you like.\n",
    "vocab_path = os.path.join(save_dir, \"vocab.pkl\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "tokenizer.save_vocab(vocab_path)\n",
    "print(f\"Vocabulary saved to {vocab_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ac026",
   "metadata": {},
   "source": [
    "#### Read vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1e0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read vocab\n",
    "# this step is only necessary if loading the vocab from a file instead of creating one\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b49bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " '<SOS>': 2,\n",
       " '<EOS>': 3,\n",
       " '<LINE_BREAK>': 4,\n",
       " '<SONG_BREAK>': 5,\n",
       " 'i': 6,\n",
       " 'you': 7,\n",
       " 'the': 8,\n",
       " 'and': 9,\n",
       " 'to': 10,\n",
       " 'a': 11,\n",
       " 'me': 12,\n",
       " 'my': 13,\n",
       " 'it': 14,\n",
       " 'in': 15,\n",
       " \"i'm\": 16,\n",
       " 'that': 17,\n",
       " 'your': 18,\n",
       " 'on': 19,\n",
       " 'of': 20,\n",
       " 'all': 21,\n",
       " 'but': 22,\n",
       " \"don't\": 23,\n",
       " 'we': 24,\n",
       " 'like': 25,\n",
       " 'know': 26,\n",
       " 'be': 27,\n",
       " 'for': 28,\n",
       " 'when': 29,\n",
       " 'is': 30,\n",
       " 'so': 31,\n",
       " \"it's\": 32,\n",
       " 'just': 33,\n",
       " 'with': 34,\n",
       " 'this': 35,\n",
       " 'love': 36,\n",
       " 'what': 37,\n",
       " \"you're\": 38,\n",
       " 'was': 39,\n",
       " 'got': 40,\n",
       " 'up': 41,\n",
       " 'if': 42,\n",
       " 'do': 43,\n",
       " 'never': 44,\n",
       " 'no': 45,\n",
       " 'not': 46,\n",
       " 'out': 47,\n",
       " \"'cause\": 48,\n",
       " 'can': 49,\n",
       " \"can't\": 50,\n",
       " 'now': 51,\n",
       " 'wanna': 52,\n",
       " 'time': 53,\n",
       " 'she': 54,\n",
       " 'one': 55,\n",
       " 'get': 56,\n",
       " 'go': 57,\n",
       " 'me,': 58,\n",
       " 'are': 59,\n",
       " 'at': 60,\n",
       " 'see': 61,\n",
       " 'want': 62,\n",
       " 'oh,': 63,\n",
       " 'say': 64,\n",
       " 'let': 65,\n",
       " 'from': 66,\n",
       " 'they': 67,\n",
       " 'make': 68,\n",
       " 'take': 69,\n",
       " 'yeah,': 70,\n",
       " 'been': 71,\n",
       " 'have': 72,\n",
       " 'you,': 73,\n",
       " \"i'll\": 74,\n",
       " 'yeah': 75,\n",
       " 'tell': 76,\n",
       " 'feel': 77,\n",
       " \"i've\": 78,\n",
       " 'could': 79,\n",
       " 'way': 80,\n",
       " 'down': 81,\n",
       " 'baby,': 82,\n",
       " 'need': 83,\n",
       " 'how': 84,\n",
       " 'her': 85,\n",
       " 'as': 86,\n",
       " \"ain't\": 87,\n",
       " 'back': 88,\n",
       " 'come': 89,\n",
       " '(': 90,\n",
       " 'gonna': 91,\n",
       " 'were': 92,\n",
       " 'think': 93,\n",
       " 'right': 94,\n",
       " 'he': 95,\n",
       " 'here': 96,\n",
       " 'will': 97,\n",
       " 'too': 98,\n",
       " \"we're\": 99,\n",
       " 'every': 100,\n",
       " 'give': 101,\n",
       " 'life': 102,\n",
       " 'there': 103,\n",
       " 'night': 104,\n",
       " 'away': 105,\n",
       " 'only': 106,\n",
       " 'then': 107,\n",
       " 'oh': 108,\n",
       " 'ooh,': 109,\n",
       " \"there's\": 110,\n",
       " 'by': 111,\n",
       " 'about': 112,\n",
       " 'through': 113,\n",
       " 'our': 114,\n",
       " 'heart': 115,\n",
       " 'where': 116,\n",
       " 'it,': 117,\n",
       " 'said': 118,\n",
       " 'look': 119,\n",
       " 'eyes': 120,\n",
       " 'keep': 121,\n",
       " 'who': 122,\n",
       " 'why': 123,\n",
       " 'ever': 124,\n",
       " 'things': 125,\n",
       " 'more': 126,\n",
       " \"that's\": 127,\n",
       " ',': 128,\n",
       " 'still': 129,\n",
       " 'had': 130,\n",
       " 'baby': 131,\n",
       " 'would': 132,\n",
       " 'little': 133,\n",
       " 'or': 134,\n",
       " 'better': 135,\n",
       " 'girl': 136,\n",
       " 'did': 137,\n",
       " 'us': 138,\n",
       " 'always': 139,\n",
       " \"won't\": 140,\n",
       " 'mind': 141,\n",
       " 'call': 142,\n",
       " 'really': 143,\n",
       " 'even': 144,\n",
       " 'good': 145,\n",
       " 'off': 146,\n",
       " 'hold': 147,\n",
       " 'am': 148,\n",
       " \"i'd\": 149,\n",
       " 'some': 150,\n",
       " 'world': 151,\n",
       " 'made': 152,\n",
       " 'an': 153,\n",
       " 'into': 154,\n",
       " 'home': 155,\n",
       " 'over': 156,\n",
       " 'put': 157,\n",
       " 'everything': 158,\n",
       " 'no,': 159,\n",
       " 'long': 160,\n",
       " 'bad': 161,\n",
       " 'day': 162,\n",
       " 'leave': 163,\n",
       " 'again': 164,\n",
       " 'gotta': 165,\n",
       " 'before': 166,\n",
       " 'alone': 167,\n",
       " 'man': 168,\n",
       " 'now,': 169,\n",
       " \"she's\": 170,\n",
       " 'find': 171,\n",
       " 'down,': 172,\n",
       " 'than': 173,\n",
       " 'around': 174,\n",
       " 'show': 175,\n",
       " 'myself': 176,\n",
       " 'another': 177,\n",
       " 'wish': 178,\n",
       " 'hard': 179,\n",
       " 'turn': 180,\n",
       " 'thing': 181,\n",
       " 'something': 182,\n",
       " 'them': 183,\n",
       " \"you'll\": 184,\n",
       " \"you've\": 185,\n",
       " 'his': 186,\n",
       " 'used': 187,\n",
       " \"'til\": 188,\n",
       " 'up,': 189,\n",
       " 'place': 190,\n",
       " 'without': 191,\n",
       " 'last': 192,\n",
       " 'found': 193,\n",
       " 'run': 194,\n",
       " 'thought': 195,\n",
       " 'fall': 196,\n",
       " 'should': 197,\n",
       " 'head': 198,\n",
       " 'much': 199,\n",
       " 'hear': 200,\n",
       " 'face': 201,\n",
       " 'new': 202,\n",
       " 'girl,': 203,\n",
       " 'him': 204,\n",
       " 'nothing': 205,\n",
       " 'inside': 206,\n",
       " 'first': 207,\n",
       " 'light': 208,\n",
       " 'live': 209,\n",
       " 'tonight': 210,\n",
       " 'left': 211,\n",
       " 'ooh': 212,\n",
       " 'these': 213,\n",
       " 'someone': 214,\n",
       " 'stay': 215,\n",
       " 'might': 216,\n",
       " \"we'll\": 217,\n",
       " 'try': 218,\n",
       " 'remember': 219,\n",
       " 'friends': 220,\n",
       " 'know,': 221,\n",
       " 'end': 222,\n",
       " 'knew': 223,\n",
       " 'best': 224,\n",
       " 'believe': 225,\n",
       " 'maybe': 226,\n",
       " \"'em\": 227,\n",
       " 'change': 228,\n",
       " 'own': 229,\n",
       " 'real': 230,\n",
       " 'told': 231,\n",
       " 'stop': 232,\n",
       " \"didn't\": 233,\n",
       " 'took': 234,\n",
       " 'care': 235,\n",
       " \"let's\": 236,\n",
       " 'shit': 237,\n",
       " 'forever': 238,\n",
       " 'well,': 239,\n",
       " 'gone': 240,\n",
       " 'hey,': 241,\n",
       " 'wait': 242,\n",
       " 'love,': 243,\n",
       " 'on,': 244,\n",
       " \"'bout\": 245,\n",
       " 'mine': 246,\n",
       " 'while': 247,\n",
       " 'said,': 248,\n",
       " 'lost': 249,\n",
       " 'tryna': 250,\n",
       " 'miss': 251,\n",
       " 'kiss': 252,\n",
       " 'name': 253,\n",
       " 'hate': 254,\n",
       " 'enough': 255,\n",
       " 'die': 256,\n",
       " 'their': 257,\n",
       " 'dance': 258,\n",
       " 'cry': 259,\n",
       " 'break': 260,\n",
       " 'say,': 261,\n",
       " 'sky': 262,\n",
       " 'fuck': 263,\n",
       " 'hope': 264,\n",
       " 'touch': 265,\n",
       " 'people': 266,\n",
       " 'hit': 267,\n",
       " 'walk': 268,\n",
       " 'side': 269,\n",
       " 'guess': 270,\n",
       " 'young': 271,\n",
       " \"nothin'\": 272,\n",
       " \"he's\": 273,\n",
       " 'done': 274,\n",
       " 'fire': 275,\n",
       " 'everybody': 276,\n",
       " 'else': 277,\n",
       " 'same': 278,\n",
       " 'out,': 279,\n",
       " 'feeling': 280,\n",
       " \"somethin'\": 281,\n",
       " 'feels': 282,\n",
       " 'me?': 283,\n",
       " 'hands': 284,\n",
       " 'under': 285,\n",
       " 'dark': 286,\n",
       " 'wrong': 287,\n",
       " 'cold': 288,\n",
       " 'talk': 289,\n",
       " 'play': 290,\n",
       " 'go,': 291,\n",
       " 'free': 292,\n",
       " 'hand': 293,\n",
       " 'somebody': 294,\n",
       " 'crazy': 295,\n",
       " 'seen': 296,\n",
       " 'two': 297,\n",
       " 'both': 298,\n",
       " 'true': 299,\n",
       " 'i,': 300,\n",
       " 'ah': 301,\n",
       " 'old': 302,\n",
       " 'song': 303,\n",
       " 'body': 304,\n",
       " 'watch': 305,\n",
       " 'dream': 306,\n",
       " 'sun': 307,\n",
       " \"what's\": 308,\n",
       " 'other': 309,\n",
       " 'help': 310,\n",
       " 'heard': 311,\n",
       " \"you'd\": 312,\n",
       " 'rain': 313,\n",
       " 'start': 314,\n",
       " 'late': 315,\n",
       " 'mean': 316,\n",
       " \"feelin'\": 317,\n",
       " 'saw': 318,\n",
       " 'makes': 319,\n",
       " \"gon'\": 320,\n",
       " 'pretty': 321,\n",
       " 'close': 322,\n",
       " 'nobody': 323,\n",
       " 'please': 324,\n",
       " 'lose': 325,\n",
       " 'high': 326,\n",
       " 'has': 327,\n",
       " 'time,': 328,\n",
       " 'la,': 329,\n",
       " 'big': 330,\n",
       " 'god': 331,\n",
       " 'goes': 332,\n",
       " 'until': 333,\n",
       " 'days': 334,\n",
       " 'wanted': 335,\n",
       " \"goin'\": 336,\n",
       " 'falling': 337,\n",
       " 'door': 338,\n",
       " 'pain': 339,\n",
       " 'girls': 340,\n",
       " 'because': 341,\n",
       " 'one,': 342,\n",
       " 'lie': 343,\n",
       " 'night,': 344,\n",
       " 'way,': 345,\n",
       " 'ya': 346,\n",
       " 'tried': 347,\n",
       " 'getting': 348,\n",
       " 'those': 349,\n",
       " 'hey': 350,\n",
       " \"they're\": 351,\n",
       " 'words': 352,\n",
       " 'bring': 353,\n",
       " 'came': 354,\n",
       " 'gave': 355,\n",
       " \"couldn't\": 356,\n",
       " 'together': 357,\n",
       " 'that,': 358,\n",
       " 'man,': 359,\n",
       " 'boy': 360,\n",
       " 'sorry': 361,\n",
       " 'waiting': 362,\n",
       " 'floor': 363,\n",
       " 'far': 364,\n",
       " 'knows': 365,\n",
       " 'since': 366,\n",
       " \"we've\": 367,\n",
       " ')': 368,\n",
       " 'after': 369,\n",
       " 'black': 370,\n",
       " 'matter': 371,\n",
       " 'alright': 372,\n",
       " 'sweet': 373,\n",
       " 'room': 374,\n",
       " 'lights': 375,\n",
       " 'scared': 376,\n",
       " 'hell': 377,\n",
       " 'takes': 378,\n",
       " 'ah,': 379,\n",
       " 'fell': 380,\n",
       " 'deep': 381,\n",
       " 'looking': 382,\n",
       " 'must': 383,\n",
       " 'hot': 384,\n",
       " 'bitch': 385,\n",
       " \"i'ma\": 386,\n",
       " \"doesn't\": 387,\n",
       " 'apart': 388,\n",
       " 'smile': 389,\n",
       " 'pull': 390,\n",
       " 'money': 391,\n",
       " 'rock': 392,\n",
       " 'next': 393,\n",
       " 'tears': 394,\n",
       " 'open': 395,\n",
       " 'kind': 396,\n",
       " 'different': 397,\n",
       " 'caught': 398,\n",
       " 'like,': 399,\n",
       " 'felt': 400,\n",
       " 'dreams': 401,\n",
       " 'sound': 402,\n",
       " 'friend': 403,\n",
       " 'ready': 404,\n",
       " 'met': 405,\n",
       " 'boy,': 406,\n",
       " 'though': 407,\n",
       " 'hurt': 408,\n",
       " 'broken': 409,\n",
       " 'perfect': 410,\n",
       " 'hide': 411,\n",
       " 'stars': 412,\n",
       " 'each': 413,\n",
       " 'going': 414,\n",
       " 'comes': 415,\n",
       " 'sing': 416,\n",
       " 'alive': 417,\n",
       " 'seems': 418,\n",
       " 'shine': 419,\n",
       " 'trying': 420,\n",
       " 'la': 421,\n",
       " 'million': 422,\n",
       " 'sure': 423,\n",
       " 'stand': 424,\n",
       " \"isn't\": 425,\n",
       " 'mind,': 426,\n",
       " 'sometimes': 427,\n",
       " 'loved': 428,\n",
       " 'work': 429,\n",
       " 'save': 430,\n",
       " 'ride': 431,\n",
       " \"livin'\": 432,\n",
       " 'drink': 433,\n",
       " 'sleep': 434,\n",
       " 'away,': 435,\n",
       " \"comin'\": 436,\n",
       " 'many': 437,\n",
       " 'anything': 438,\n",
       " 'once': 439,\n",
       " 'okay': 440,\n",
       " 'happy': 441,\n",
       " 'am,': 442,\n",
       " 'times': 443,\n",
       " 'you?': 444,\n",
       " 'town': 445,\n",
       " 'blue': 446,\n",
       " 'any': 447,\n",
       " 'fine': 448,\n",
       " 'heaven': 449,\n",
       " 'went': 450,\n",
       " 'drive': 451,\n",
       " 'step': 452,\n",
       " 'guy': 453,\n",
       " 'phone': 454,\n",
       " 'easy': 455,\n",
       " 'coming': 456,\n",
       " 'known': 457,\n",
       " 'whole': 458,\n",
       " 'fly': 459,\n",
       " 'blow': 460,\n",
       " 'road': 461,\n",
       " 'control': 462,\n",
       " 'boys': 463,\n",
       " 'good,': 464,\n",
       " 'well': 465,\n",
       " 'beat': 466,\n",
       " 'stuck': 467,\n",
       " 'thе': 468,\n",
       " 'summer': 469,\n",
       " 'today': 470,\n",
       " 'bed': 471,\n",
       " 'so,': 472,\n",
       " 'wonder': 473,\n",
       " 'red': 474,\n",
       " \"fuckin'\": 475,\n",
       " 'and,': 476,\n",
       " 'does': 477,\n",
       " 'work,': 478,\n",
       " 'anymore': 479,\n",
       " 'line': 480,\n",
       " 'do,': 481,\n",
       " \"lookin'\": 482,\n",
       " 'now?': 483,\n",
       " 'yourself': 484,\n",
       " 'dancing': 485,\n",
       " 'blame': 486,\n",
       " 'fun': 487,\n",
       " 'gets': 488,\n",
       " 'babe': 489,\n",
       " \"don't,\": 490,\n",
       " 'white': 491,\n",
       " 'it?': 492,\n",
       " 'beautiful': 493,\n",
       " 'slow': 494,\n",
       " 'between': 495,\n",
       " 'fight': 496,\n",
       " 'tired': 497,\n",
       " 'kinda': 498,\n",
       " 'living': 499,\n",
       " \"wasn't\": 500,\n",
       " 'may': 501,\n",
       " 'breathe': 502,\n",
       " 'hundred': 503,\n",
       " 'wake': 504,\n",
       " 'meant': 505,\n",
       " 'ayy,': 506,\n",
       " 'such': 507,\n",
       " 'ways': 508,\n",
       " 'set': 509,\n",
       " 'move': 510,\n",
       " 'years': 511,\n",
       " 'wants': 512,\n",
       " 'thinking': 513,\n",
       " 'city': 514,\n",
       " 'skin': 515,\n",
       " \"they'll\": 516,\n",
       " 'blood': 517,\n",
       " \"fallin'\": 518,\n",
       " 'kill': 519,\n",
       " 'moment': 520,\n",
       " 'swear': 521,\n",
       " 'ask': 522,\n",
       " 'life,': 523,\n",
       " 'read': 524,\n",
       " 'party': 525,\n",
       " 'yes,': 526,\n",
       " 'act': 527,\n",
       " \"gettin'\": 528,\n",
       " 'everyone': 529,\n",
       " \"runnin'\": 530,\n",
       " \"talkin'\": 531,\n",
       " \"should've\": 532,\n",
       " 'damn': 533,\n",
       " 'knock': 534,\n",
       " 'cut': 535,\n",
       " 'fast': 536,\n",
       " 'strong': 537,\n",
       " 'forget': 538,\n",
       " 'already': 539,\n",
       " 'breath': 540,\n",
       " 'back,': 541,\n",
       " \"everybody's\": 542,\n",
       " 'started': 543,\n",
       " 'dead': 544,\n",
       " 'in,': 545,\n",
       " 'doing': 546,\n",
       " 'ayy': 547,\n",
       " 'burn': 548,\n",
       " 'sad': 549,\n",
       " 'cool': 550,\n",
       " 'house': 551,\n",
       " \"holdin'\": 552,\n",
       " 'cannot': 553,\n",
       " 'past': 554,\n",
       " 'uh,': 555,\n",
       " '\"i': 556,\n",
       " 'truth': 557,\n",
       " 'two,': 558,\n",
       " 'called': 559,\n",
       " 'loves': 560,\n",
       " 'but,': 561,\n",
       " 'single': 562,\n",
       " 'turned': 563,\n",
       " 'four': 564,\n",
       " 'understand': 565,\n",
       " 'nights': 566,\n",
       " 'bit': 567,\n",
       " 'na-na,': 568,\n",
       " 'hello,': 569,\n",
       " 'mmm': 570,\n",
       " 'woah': 571,\n",
       " \"who's\": 572,\n",
       " 'lot': 573,\n",
       " 'soul': 574,\n",
       " 'very': 575,\n",
       " 'my,': 576,\n",
       " 'afraid': 577,\n",
       " 'tonight,': 578,\n",
       " 'dog': 579,\n",
       " \"lovin'\": 580,\n",
       " 'running': 581,\n",
       " 'car': 582,\n",
       " 'music': 583,\n",
       " 'bet': 584,\n",
       " 'fear': 585,\n",
       " 'niggas': 586,\n",
       " 'seem': 587,\n",
       " \"thinkin'\": 588,\n",
       " 'chance': 589,\n",
       " 'arms': 590,\n",
       " 'send': 591,\n",
       " 'lies': 592,\n",
       " 'pay': 593,\n",
       " 'ones': 594,\n",
       " \"doin'\": 595,\n",
       " 'heart,': 596,\n",
       " 'throw': 597,\n",
       " 'rest': 598,\n",
       " 'looked': 599,\n",
       " 'says': 600,\n",
       " 'around,': 601,\n",
       " 'three': 602,\n",
       " 'yo': 603,\n",
       " 'feet': 604,\n",
       " 'ends': 605,\n",
       " 'ocean': 606,\n",
       " 'check': 607,\n",
       " 'favorite': 608,\n",
       " 'loving': 609,\n",
       " 'trouble': 610,\n",
       " 'lonely': 611,\n",
       " 'rather': 612,\n",
       " 'quite': 613,\n",
       " 'promise': 614,\n",
       " 'hair': 615,\n",
       " 'mad': 616,\n",
       " 'again,': 617,\n",
       " 'oh-oh,': 618,\n",
       " 'nigga': 619,\n",
       " 'meet': 620,\n",
       " 'least': 621,\n",
       " 'hearts': 622,\n",
       " 'losing': 623,\n",
       " 'goodbye': 624,\n",
       " 'top': 625,\n",
       " 'along': 626,\n",
       " 'welcome': 627,\n",
       " 'bubble': 628,\n",
       " 'born': 629,\n",
       " 'means': 630,\n",
       " 'being': 631,\n",
       " 'its': 632,\n",
       " 'calling': 633,\n",
       " 'street': 634,\n",
       " 'mama': 635,\n",
       " 'air': 636,\n",
       " 'follow': 637,\n",
       " 'im': 638,\n",
       " 'worth': 639,\n",
       " 'walked': 640,\n",
       " 'money,': 641,\n",
       " 'radio': 642,\n",
       " 'outside': 643,\n",
       " 'lay': 644,\n",
       " 'gold': 645,\n",
       " 'kids': 646,\n",
       " \"wouldn't\": 647,\n",
       " 'full': 648,\n",
       " 'keeps': 649,\n",
       " 'sometimes,': 650,\n",
       " 'dirty': 651,\n",
       " 'woman': 652,\n",
       " 'behind': 653,\n",
       " 'nice': 654,\n",
       " 'buy': 655,\n",
       " 'forgive': 656,\n",
       " 'sign': 657,\n",
       " 'wood': 658,\n",
       " 'broke': 659,\n",
       " 'likes': 660,\n",
       " 'uh': 661,\n",
       " 'shake': 662,\n",
       " 'age': 663,\n",
       " 'choose': 664,\n",
       " 'go?': 665,\n",
       " 'word': 666,\n",
       " 'giving': 667,\n",
       " 'water': 668,\n",
       " 'scream': 669,\n",
       " 'ground': 670,\n",
       " 'looks': 671,\n",
       " 'taking': 672,\n",
       " 'oh-oh': 673,\n",
       " 'low': 674,\n",
       " 'us,': 675,\n",
       " 'trust': 676,\n",
       " 'this,': 677,\n",
       " 'lead': 678,\n",
       " \"we'd\": 679,\n",
       " 'somewhere': 680,\n",
       " 'part': 681,\n",
       " 'against': 682,\n",
       " 'game': 683,\n",
       " 'knees': 684,\n",
       " 'lives': 685,\n",
       " 'god,': 686,\n",
       " 'walls': 687,\n",
       " 'drop': 688,\n",
       " 'making': 689,\n",
       " 'cause': 690,\n",
       " 'straight': 691,\n",
       " 'wild': 692,\n",
       " 'whatever': 693,\n",
       " 'power': 694,\n",
       " 'paradise': 695,\n",
       " 'pick': 696,\n",
       " 'few': 697,\n",
       " \"'round\": 698,\n",
       " 'day,': 699,\n",
       " 'bubble,': 700,\n",
       " 'almost': 701,\n",
       " 'there,': 702,\n",
       " 'tomorrow': 703,\n",
       " 'second': 704,\n",
       " \"makin'\": 705,\n",
       " 'crazy,': 706,\n",
       " 'door,': 707,\n",
       " 'voice': 708,\n",
       " 'mm,': 709,\n",
       " \"shouldn't\": 710,\n",
       " 'asked': 711,\n",
       " 'laughed': 712,\n",
       " 'sounds': 713,\n",
       " 'fate': 714,\n",
       " 'na,': 715,\n",
       " \"dancin'\": 716,\n",
       " 'ooh-ooh,': 717,\n",
       " 'dance,': 718,\n",
       " 'clear': 719,\n",
       " 'played': 720,\n",
       " 'changed': 721,\n",
       " 'speak': 722,\n",
       " 'okay,': 723,\n",
       " \"tryin'\": 724,\n",
       " 'soon': 725,\n",
       " 'learned': 726,\n",
       " 'grave': 727,\n",
       " 'loud': 728,\n",
       " 'needed': 729,\n",
       " 'sit': 730,\n",
       " 'showed': 731,\n",
       " 'land': 732,\n",
       " 'ice': 733,\n",
       " 'see,': 734,\n",
       " 'front': 735,\n",
       " 'reasons': 736,\n",
       " 'wind': 737,\n",
       " 'drunk': 738,\n",
       " 'ooh-ooh': 739,\n",
       " 'dreaming': 740,\n",
       " 'eyes,': 741,\n",
       " 'lover': 742,\n",
       " 'mess': 743,\n",
       " 'learn': 744,\n",
       " 'anyone': 745,\n",
       " 'gun': 746,\n",
       " 'half': 747,\n",
       " 'spend': 748,\n",
       " 'thank': 749,\n",
       " 'listen': 750,\n",
       " 'mr.': 751,\n",
       " 'picture': 752,\n",
       " 'slowly': 753,\n",
       " 'empty': 754,\n",
       " 'mark': 755,\n",
       " \"watchin'\": 756,\n",
       " \"wishin'\": 757,\n",
       " 'streets': 758,\n",
       " 'held': 759,\n",
       " 'fucking': 760,\n",
       " 'club': 761,\n",
       " 'woah,': 762,\n",
       " 'three,': 763,\n",
       " 'ay': 764,\n",
       " 'te': 765,\n",
       " 'breaking': 766,\n",
       " 'silence': 767,\n",
       " 'lips': 768,\n",
       " 'hurts': 769,\n",
       " 'catch': 770,\n",
       " 'pass': 771,\n",
       " \"haven't\": 772,\n",
       " 'world,': 773,\n",
       " 'honey,': 774,\n",
       " 'know?': 775,\n",
       " 'eye': 776,\n",
       " 'morning': 777,\n",
       " 'here,': 778,\n",
       " 'shoot': 779,\n",
       " 'missing': 780,\n",
       " 'tight': 781,\n",
       " 'type': 782,\n",
       " \"walkin'\": 783,\n",
       " 'write': 784,\n",
       " 'bought': 785,\n",
       " 'standing': 786,\n",
       " 'gimme': 787,\n",
       " 'damn,': 788,\n",
       " 'admit': 789,\n",
       " 'high,': 790,\n",
       " 'fingers': 791,\n",
       " 'cry,': 792,\n",
       " 'care,': 793,\n",
       " 'ass': 794,\n",
       " 'wait,': 795,\n",
       " 'tear': 796,\n",
       " 'myself,': 797,\n",
       " 'hands,': 798,\n",
       " 'ten': 799,\n",
       " 'none': 800,\n",
       " 'grow': 801,\n",
       " 'space': 802,\n",
       " 'longer': 803,\n",
       " 'all,': 804,\n",
       " 'angel': 805,\n",
       " 'sick': 806,\n",
       " 'finally': 807,\n",
       " 'mouth': 808,\n",
       " 'peace': 809,\n",
       " 'year': 810,\n",
       " 'middle': 811,\n",
       " 'wall': 812,\n",
       " 'burning': 813,\n",
       " 'face,': 814,\n",
       " \"singin'\": 815,\n",
       " 'mistake': 816,\n",
       " 'couple': 817,\n",
       " 'places': 818,\n",
       " 'friday': 819,\n",
       " 'popular': 820,\n",
       " 'fucked': 821,\n",
       " 'window': 822,\n",
       " 'hang': 823,\n",
       " 'upon': 824,\n",
       " 'better,': 825,\n",
       " 'mama,': 826,\n",
       " 'love?': 827,\n",
       " 'thousand': 828,\n",
       " 'memory': 829,\n",
       " 'again?': 830,\n",
       " 'wrong,': 831,\n",
       " 'closer': 832,\n",
       " 'become': 833,\n",
       " 'anybody': 834,\n",
       " 'real,': 835,\n",
       " \"bein'\": 836,\n",
       " 'trouble,': 837,\n",
       " 'fighting': 838,\n",
       " 'then,': 839,\n",
       " '),': 840,\n",
       " 'price': 841,\n",
       " 'laugh': 842,\n",
       " 'taste': 843,\n",
       " 'mm': 844,\n",
       " 'pray': 845,\n",
       " 'mir': 846,\n",
       " 'opened': 847,\n",
       " 'sat': 848,\n",
       " 'are,': 849,\n",
       " 'above': 850,\n",
       " 'reason': 851,\n",
       " 'holding': 852,\n",
       " 'roses': 853,\n",
       " 'american': 854,\n",
       " 'brain': 855,\n",
       " 'woke': 856,\n",
       " 'wear': 857,\n",
       " \"havin'\": 858,\n",
       " 'five': 859,\n",
       " 'hoppus': 860,\n",
       " 'gone,': 861,\n",
       " \"y'all\": 862,\n",
       " \"darlin',\": 863,\n",
       " 'outta': 864,\n",
       " 'sexy': 865,\n",
       " 'ni': 866,\n",
       " 'watching': 867,\n",
       " 'pretend': 868,\n",
       " 'count': 869,\n",
       " 'move,': 870,\n",
       " 'yet': 871,\n",
       " 'belong': 872,\n",
       " 'carry': 873,\n",
       " 'small': 874,\n",
       " 'ghost': 875,\n",
       " 'diamond': 876,\n",
       " 'ring': 877,\n",
       " 'sea': 878,\n",
       " 'you\"': 879,\n",
       " 'point': 880,\n",
       " 'saying': 881,\n",
       " 'right,': 882,\n",
       " 'fake': 883,\n",
       " 'roll': 884,\n",
       " 'dust': 885,\n",
       " 'magic': 886,\n",
       " 'shit,': 887,\n",
       " 'key': 888,\n",
       " 'crowd': 889,\n",
       " 'glass': 890,\n",
       " 'star': 891,\n",
       " 'dying': 892,\n",
       " 'bad,': 893,\n",
       " 'say?': 894,\n",
       " 'quiet': 895,\n",
       " 'city,': 896,\n",
       " 'twice': 897,\n",
       " 'long,': 898,\n",
       " 'dress': 899,\n",
       " 'off,': 900,\n",
       " \"standin'\": 901,\n",
       " 'head,': 902,\n",
       " 'shame': 903,\n",
       " 'bitch,': 904,\n",
       " 'sex': 905,\n",
       " 'fit': 906,\n",
       " 'que': 907,\n",
       " 'bones': 908,\n",
       " 'across': 909,\n",
       " 'mean?': 910,\n",
       " 'ah-ah,': 911,\n",
       " 'piece': 912,\n",
       " 'saved': 913,\n",
       " 'earth': 914,\n",
       " 'wide': 915,\n",
       " 'that?': 916,\n",
       " 'mmm,': 917,\n",
       " 'feelings': 918,\n",
       " 'mirror': 919,\n",
       " 'lady': 920,\n",
       " 'cold,': 921,\n",
       " 'death': 922,\n",
       " 'probably': 923,\n",
       " 'family': 924,\n",
       " 'six': 925,\n",
       " 'clothes': 926,\n",
       " 'push': 927,\n",
       " 'tongue': 928,\n",
       " 'pictures': 929,\n",
       " 'fool': 930,\n",
       " 'eat': 931,\n",
       " \"aren't\": 932,\n",
       " 'ha': 933,\n",
       " 'less': 934,\n",
       " 'run,': 935,\n",
       " 'sun,': 936,\n",
       " 'much,': 937,\n",
       " 'miles': 938,\n",
       " 'kissed': 939,\n",
       " 'games': 940,\n",
       " 'built': 941,\n",
       " 'bleed': 942,\n",
       " 'regret': 943,\n",
       " 'turns': 944,\n",
       " 'somehow': 945,\n",
       " 'higher': 946,\n",
       " 'edge': 947,\n",
       " 'her,': 948,\n",
       " \"cryin'\": 949,\n",
       " 'do?': 950,\n",
       " 'band': 951,\n",
       " 'blind': 952,\n",
       " 'underneath': 953,\n",
       " 'talking': 954,\n",
       " 'young,': 955,\n",
       " 'everywhere': 956,\n",
       " 'chest': 957,\n",
       " \"tellin'\": 958,\n",
       " 'days,': 959,\n",
       " 'want,': 960,\n",
       " 'mine,': 961,\n",
       " 'reach': 962,\n",
       " 'faith': 963,\n",
       " 'pieces': 964,\n",
       " 'over,': 965,\n",
       " \"takin'\": 966,\n",
       " 'dad': 967,\n",
       " 'alive,': 968,\n",
       " 'asking': 969,\n",
       " 'son': 970,\n",
       " 'devil': 971,\n",
       " 'na': 972,\n",
       " 'own,': 973,\n",
       " 'ass,': 974,\n",
       " 'youth': 975,\n",
       " 'fire,': 976,\n",
       " 'kid': 977,\n",
       " 'bright': 978,\n",
       " 'pop': 979,\n",
       " 'beside': 980,\n",
       " 'nigga,': 981,\n",
       " 'yo,': 982,\n",
       " 'uh-huh': 983,\n",
       " 'eh': 984,\n",
       " 'story': 985,\n",
       " 'win': 986,\n",
       " 'before,': 987,\n",
       " 'use': 988,\n",
       " 'turning': 989,\n",
       " 'moon': 990,\n",
       " 'school': 991,\n",
       " 'bite': 992,\n",
       " 'crossed': 993,\n",
       " 'share': 994,\n",
       " 'waste': 995,\n",
       " 'close,': 996,\n",
       " 'locked': 997,\n",
       " 'neck': 998,\n",
       " 'diamonds': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['word_to_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f693192",
   "metadata": {},
   "source": [
    "#### Get token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd86e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text length: 103967 tokens\n",
      "The number of unique tokens in the vocab is: 3541\n"
     ]
    }
   ],
   "source": [
    "# Encode text\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Encoded text length: {len(token_ids)} tokens\")\n",
    "print(\"The number of unique tokens in the vocab is:\", len(set(token_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a07bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: hello, it's me i was wondering if after all these years, you'd like to meet to go over everything th\n"
     ]
    }
   ],
   "source": [
    "ids_to_words = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded text: {ids_to_words[:100]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd27c60",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f1611",
   "metadata": {},
   "source": [
    "#### Create data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3531c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "data_module = TextDataModule(\n",
    "    token_ids=token_ids,\n",
    "    sequence_length=sequence_length,\n",
    "    stride=stride,\n",
    "    batch_size=batch_size,\n",
    "    train_split=train_split,\n",
    "    val_split=val_split,\n",
    "    num_workers=int(os.cpu_count()*0.8) #use half of the cores   #num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25cbd6",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e160cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = TransformerLightningModule(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf90a3",
   "metadata": {},
   "source": [
    "#### Set callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0db2eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    patience=patience,\n",
    "    min_delta=min_delta,\n",
    "    mode=mode,\n",
    "    verbose=True\n",
    ")\n",
    "callbacks.append(early_stopping)\n",
    "\n",
    "\n",
    "# Model checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=save_dir,\n",
    "    filename=f\"{experiment_name}-epoch={{epoch:02d}}-val_loss={{val/loss:.3f}}\", #-v{trainer.logger.version:02d}\",\n",
    "    monitor=monitor,\n",
    "    mode=mode,\n",
    "    auto_insert_metric_name=False, # Prevents the name 'val/loss=' from being prepended\n",
    "    save_top_k=save_top_k,\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "# Learning rate monitoring\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "callbacks.append(lr_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b673769",
   "metadata": {},
   "source": [
    "#### Create trainer and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e196f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "# Create logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=log_dir,\n",
    "    name=experiment_name,\n",
    "    version=None\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    precision=precision,\n",
    "    max_epochs=max_epochs,\n",
    "    gradient_clip_val=gradient_clip_val,\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    log_every_n_steps=log_every_n_steps,\n",
    "    val_check_interval=val_check_interval,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    deterministic=True,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c1c0b",
   "metadata": {},
   "source": [
    "#### Summarize prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0118a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary:\n",
      "Vocabulary size: 4076\n",
      "Model parameters: 20,768\n",
      "Trainable parameters: 20,768\n",
      "Data sizes:\n",
      "Total tokens: 103967\n",
      "Sequence length: 64\n",
      "Stride: 1\n",
      "Train split: 0.7\n",
      "Val split: 0.2\n",
      "Train tokens: 72776\n",
      "Val tokens: 20794\n",
      "Test tokens: 10397\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(\"\\nModel Summary:\")\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Check data sizes\n",
    "print(\"Data sizes:\")\n",
    "print(f\"Total tokens: {len(token_ids)}\")\n",
    "print(f\"Sequence length: {sequence_length}\")\n",
    "print(f\"Stride: {stride}\")\n",
    "print(f\"Train split: {train_split}\")\n",
    "print(f\"Val split: {val_split}\")\n",
    "\n",
    "# Calculate split sizes\n",
    "total_len = len(token_ids)\n",
    "train_end = int(total_len * train_split)\n",
    "val_end = int(total_len * (train_split + val_split))\n",
    "\n",
    "print(f\"Train tokens: {train_end}\")\n",
    "print(f\"Val tokens: {val_end - train_end}\")\n",
    "print(f\"Test tokens: {total_len - val_end}\")\n",
    "\n",
    "# Check if validation data is sufficient\n",
    "val_tokens = val_end - train_end\n",
    "if val_tokens < sequence_length:\n",
    "    print(f\"WARNING: Validation data has only {val_tokens} tokens, less than sequence length {sequence_length}\")\n",
    "    print(\"This will cause validation to fail. Consider using a larger corpus or adjusting splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efc6f7",
   "metadata": {},
   "source": [
    "#### Train the neural netork transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c88831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "With the updated parameters:\n",
      "- Sequence length: 64\n",
      "- Train split: 0.7\n",
      "- Val split: 0.2\n",
      "- Monitor: val/loss\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:751: Checkpoint directory C:\\code\\data_science\\demo_llm\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | TransformerLM    | 20.8 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "20.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.8 K    Total params\n",
      "0.083     Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2273/2273 [01:48<00:00, 20.94it/s, v_num=69, train/loss_step=7.010, train/perplexity_step=1.11e+3, val/loss=7.070, val/perplexity=1.19e+3, train/loss_epoch=7.810, train/perplexity_epoch=2.95e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved. New best score: 7.067\n",
      "Epoch 0, global step 2273: 'val/loss' reached 7.06729 (best 7.06729), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=00-val_loss=7.067.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2273/2273 [02:43<00:00, 13.89it/s, v_num=69, train/loss_step=5.490, train/perplexity_step=242.0, val/loss=5.660, val/perplexity=308.0, train/loss_epoch=6.070, train/perplexity_epoch=492.0]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 1.405 >= min_delta = 0.001. New best score: 5.662\n",
      "Epoch 1, global step 4546: 'val/loss' reached 5.66237 (best 5.66237), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=01-val_loss=5.662.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2273/2273 [02:34<00:00, 14.70it/s, v_num=69, train/loss_step=5.470, train/perplexity_step=238.0, val/loss=5.550, val/perplexity=287.0, train/loss_epoch=5.520, train/perplexity_epoch=251.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.108 >= min_delta = 0.001. New best score: 5.554\n",
      "Epoch 2, global step 6819: 'val/loss' reached 5.55396 (best 5.55396), saving model to 'C:\\\\code\\\\data_science\\\\demo_llm\\\\checkpoints\\\\transformer_lm-epoch=02-val_loss=5.554.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2273/2273 [02:33<00:00, 14.84it/s, v_num=69, train/loss_step=5.380, train/perplexity_step=216.0, val/loss=5.560, val/perplexity=294.0, train/loss_epoch=5.490, train/perplexity_epoch=243.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 9092: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2273/2273 [02:33<00:00, 14.84it/s, v_num=69, train/loss_step=5.540, train/perplexity_step=255.0, val/loss=5.580, val/perplexity=303.0, train/loss_epoch=5.480, train/perplexity_epoch=241.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 11365: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2273/2273 [02:32<00:00, 14.88it/s, v_num=69, train/loss_step=5.580, train/perplexity_step=266.0, val/loss=5.600, val/perplexity=311.0, train/loss_epoch=5.480, train/perplexity_epoch=240.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 13638: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2273/2273 [02:32<00:00, 14.95it/s, v_num=69, train/loss_step=5.370, train/perplexity_step=215.0, val/loss=5.620, val/perplexity=321.0, train/loss_epoch=5.480, train/perplexity_epoch=240.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 15911: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2273/2273 [02:31<00:00, 14.98it/s, v_num=69, train/loss_step=5.240, train/perplexity_step=188.0, val/loss=5.640, val/perplexity=331.0, train/loss_epoch=5.480, train/perplexity_epoch=240.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 18184: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2273/2273 [02:48<00:00, 13.46it/s, v_num=69, train/loss_step=5.790, train/perplexity_step=328.0, val/loss=5.650, val/perplexity=342.0, train/loss_epoch=5.480, train/perplexity_epoch=240.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 20457: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2273/2273 [02:32<00:00, 14.94it/s, v_num=69, train/loss_step=5.390, train/perplexity_step=219.0, val/loss=5.670, val/perplexity=354.0, train/loss_epoch=5.480, train/perplexity_epoch=240.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 22730: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2273/2273 [02:31<00:00, 14.97it/s, v_num=69, train/loss_step=5.420, train/perplexity_step=227.0, val/loss=5.690, val/perplexity=366.0, train/loss_epoch=5.480, train/perplexity_epoch=239.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 25003: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2273/2273 [02:32<00:00, 14.90it/s, v_num=69, train/loss_step=5.270, train/perplexity_step=194.0, val/loss=5.700, val/perplexity=377.0, train/loss_epoch=5.470, train/perplexity_epoch=239.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 27276: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2273/2273 [02:33<00:00, 14.76it/s, v_num=69, train/loss_step=5.420, train/perplexity_step=225.0, val/loss=5.710, val/perplexity=386.0, train/loss_epoch=5.470, train/perplexity_epoch=239.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val/loss did not improve in the last 10 records. Best score: 5.554. Signaling Trainer to stop.\n",
      "Epoch 12, global step 29549: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2273/2273 [02:34<00:00, 14.76it/s, v_num=69, train/loss_step=5.420, train/perplexity_step=225.0, val/loss=5.710, val/perplexity=386.0, train/loss_epoch=5.470, train/perplexity_epoch=239.0]\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"With the updated parameters:\")\n",
    "print(f\"- Sequence length: {sequence_length}\")\n",
    "print(f\"- Train split: {train_split}\")\n",
    "print(f\"- Val split: {val_split}\")\n",
    "print(f\"- Monitor: {monitor}\")\n",
    "print()\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6a938",
   "metadata": {},
   "source": [
    "#### Test & Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ed8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\data_science\\demo_llm\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "print(\"\\nTesting model...\")\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Save best model instead of final model\n",
    "if checkpoint_callback.best_model_path:\n",
    "    # Copy the best model to a final location with version number\n",
    "    import shutil\n",
    "    version = trainer.logger.version\n",
    "    final_model_path = os.path.join(save_dir, f\"{experiment_name}-best-v{version:02d}.ckpt\")\n",
    "    shutil.copy2(checkpoint_callback.best_model_path, final_model_path)\n",
    "    print(f\"Best model copied to {final_model_path}\")\n",
    "    \n",
    "    # Also save as the standard final model name (overwrites previous)\n",
    "    standard_final_path = os.path.join(save_dir, f\"{experiment_name}-final.ckpt\")\n",
    "    shutil.copy2(checkpoint_callback.best_model_path, standard_final_path)\n",
    "    print(f\"Best model also saved as {standard_final_path}\")\n",
    "else:\n",
    "    print(\"No best model found, saving current model as final\")\n",
    "    version = trainer.logger.version\n",
    "    final_model_path = os.path.join(save_dir, f\"{experiment_name}-final-v{version:02d}.ckpt\")\n",
    "    trainer.save_checkpoint(final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Also save as the standard final model name (overwrites previous)\n",
    "    standard_final_path = os.path.join(save_dir, f\"{experiment_name}-final.ckpt\")\n",
    "    shutil.copy2(final_model_path, standard_final_path)\n",
    "    print(f\"Final model also saved as {standard_final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fe554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab saved to ./checkpoints\\vocab-v66.pkl\n",
      "Vocab also saved as ./checkpoints\\vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# Also save the vocab with the version number\n",
    "vocab_path = os.path.join(save_dir, f\"vocab-v{version:02d}.pkl\")\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    print(f\"Vocab saved to {vocab_path}\")\n",
    "\n",
    "# Also save as the standard vocab name (overwrites previous)\n",
    "standard_vocab_path = os.path.join(save_dir, \"vocab.pkl\")\n",
    "with open(standard_vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    print(f\"Vocab also saved as {standard_vocab_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc3ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Checkpoints saved in: ./checkpoints\n",
      "Logs saved in: ./logs\n",
      "Vocabulary saved in: ./checkpoints\\vocab-v66.pkl\n",
      "Best model: C:\\code\\data_science\\demo_llm\\checkpoints\\transformer_lm-epoch=03-val_loss=6.070.ckpt\n",
      "Best score: 6.070318698883057\n",
      "Final model saved as: ./checkpoints\\transformer_lm-best-v66.ckpt\n",
      "\n",
      "==================================================\n",
      "To run the Gradio app with your trained model:\n",
      "python -m src.app.gradio_app --model_path ./checkpoints\\transformer_lm-best-v66.ckpt --vocab_path ./checkpoints\\vocab-v66.pkl\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Checkpoints saved in: {save_dir}\")\n",
    "print(f\"Logs saved in: {log_dir}\")\n",
    "print(f\"Vocabulary saved in: {vocab_path}\")\n",
    "print(f\"Best model: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best score: {checkpoint_callback.best_model_score}\")\n",
    "print(f\"Final model saved as: {final_model_path}\")\n",
    "\n",
    "# Print instructions for running the app\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"To run the Gradio app with your trained model:\")\n",
    "print(f\"python -m src.app.gradio_app --model_path {final_model_path} --vocab_path {vocab_path}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ac8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
